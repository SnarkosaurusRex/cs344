{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "**Exercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good: {'do': 2, 'i': 2, 'like': 1, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}\n",
      "bad: {'i': 3, 'am': 2, 'spam': 2, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1}\n",
      "\n",
      "probs: \n",
      "{'do': 0.3333333333333333, 'i': 0.5, 'like': 0.3333333333333333, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01, 'am': 0.99, 'spam': 0.99, 'not': 0, 'that': 0, 'spamiam': 0}\n",
      "\n",
      "\n",
      "--- TESTS ---\n",
      " Message: I do not like green eggs and ham i do not like them Sam-I-am\n",
      "\tResult: Not spam (Probability = 0.0)\n",
      "\n",
      " Message: You can't have egg bacon spam and sausage without the spam\n",
      "\tResult: Not spam (Probability = 0.7943582510578278)\n",
      "\n",
      " Message: spam egg spam spam bacon and spam\n",
      "\tResult: SPAM (Probability = 0.9999976811325348)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module implements a Bayesian spam filter based on Paul Graham's \"A Plan for Spam\".\n",
    "Part 1 of Homework 2 for CS 344\n",
    "\n",
    "@author: ljh27\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_corpus(corpus):\n",
    "    corpus_dict = {}\n",
    "    for message in corpus:\n",
    "        for word in message:\n",
    "            #force lowercase\n",
    "            word = word.lower()\n",
    "\n",
    "            #if the word is already in the dict, increment its count by 1\n",
    "            if word in corpus_dict:\n",
    "                corpus_dict[word] += 1\n",
    "\n",
    "            #otherwise, add the word to the dict and set its count to 1\n",
    "            else:\n",
    "                corpus_dict[word] = 1\n",
    "\n",
    "    return corpus_dict\n",
    "\n",
    "\n",
    "def get_unique_words(corpus_list):\n",
    "    unique_words = []\n",
    "    for corpus in corpus_list:\n",
    "        for message in corpus:\n",
    "            for word in message:\n",
    "                word = word.lower()\n",
    "                if word not in unique_words:\n",
    "                    unique_words.append(word)\n",
    "    return unique_words\n",
    "\n",
    "\n",
    "def create_probs_dict(words, good, bad, ngood, nbad):\n",
    "    probs_dict = {}\n",
    "    for word in words:\n",
    "        if word in good:\n",
    "            g = 2 * good[word]\n",
    "        else:\n",
    "            g = 0\n",
    "        if word in bad:\n",
    "            b = bad[word]\n",
    "        else:\n",
    "            b = 0\n",
    "\n",
    "        #use a minimum count threshold of 1\n",
    "        if g+b > 1:\n",
    "            probs_dict[word] = max(0.01, min(0.99, min(1.0, b / nbad) / (min(1.0, g / ngood) + min(1.0, b / nbad))))\n",
    "        else:\n",
    "            probs_dict[word] = 0\n",
    "\n",
    "    return probs_dict\n",
    "\n",
    "\n",
    "def evaluate_msg(msg, probs):\n",
    "    prod = 1\n",
    "    comp_prod = 1\n",
    "    for word in msg:\n",
    "        if word in probs:\n",
    "            prod *= probs[word]\n",
    "            comp_prod *= (1 - probs[word])\n",
    "        else:\n",
    "            prod *= 0.4\n",
    "            comp_prod *= 0.6\n",
    "    spam_prob = prod / (prod + comp_prod)\n",
    "\n",
    "    if spam_prob > 0.9:\n",
    "        return [\"SPAM\", spam_prob]\n",
    "    else:\n",
    "        return [\"Not spam\", spam_prob]\n",
    "\n",
    "\n",
    "#Test it out!\n",
    "spam_corpus = [[\"I\", \"am\", \"spam\", \"spam\", \"I\", \"am\"], [\"I\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "ngood = len(ham_corpus)\n",
    "nbad = len(spam_corpus)\n",
    "\n",
    "good = parse_corpus(ham_corpus)\n",
    "bad = parse_corpus(spam_corpus)\n",
    "words = get_unique_words([ham_corpus, spam_corpus])\n",
    "probs = create_probs_dict(words, good, bad, ngood, nbad)\n",
    "\n",
    "print(\"good: \" + str(good))\n",
    "print(\"bad: \" + str(bad))\n",
    "print(\"\\nprobs: \")\n",
    "print(probs)\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- TESTS ---\")\n",
    "test_msg = [\"I\", \"do\", \"not\", \"like\", \"green\", \"eggs\", \"and\", \"ham\", \"i\", \"do\", \"not\", \"like\", \"them\", \"Sam-I-am\"]\n",
    "test_result = evaluate_msg(test_msg, probs)\n",
    "print(\" Message: \" + \" \".join(test_msg))\n",
    "print(\"\\tResult: \" + test_result[0] + \" (Probability = \" + str(test_result[1]) + \")\")\n",
    "\n",
    "test_msg2 = [\"You\", \"can't\", \"have\", \"egg\", \"bacon\", \"spam\", \"and\", \"sausage\", \"without\", \"the\", \"spam\"]\n",
    "test_result2 = evaluate_msg(test_msg2, probs)\n",
    "print(\"\\n Message: \" + \" \".join(test_msg2))\n",
    "print(\"\\tResult: \" + test_result2[0] + \" (Probability = \" + str(test_result2[1]) + \")\")\n",
    "\n",
    "test_msg3 = [\"spam\", \"egg\", \"spam\", \"spam\", \"bacon\", \"and\", \"spam\"]\n",
    "test_result3 = evaluate_msg(test_msg3, probs)\n",
    "print(\"\\n Message: \" + \" \".join(test_msg3))\n",
    "print(\"\\tResult: \" + test_result3[0] + \" (Probability = \" + str(test_result3[1]) + \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graham says in \"A Plan for Spam\" that this approach uses a \"Bayesian combination of the spam probabilities of individual words.\" It is also Bayesian in that it starts out by using existing data (i.e. the two corpuses) and continues to adapt by analyzing new data (i.e. new emails) as they come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 2**\n",
    "Part a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements a Bayesian network for Figure 14.12a\n",
    "Part 2 of Homework 2 for CS 344\n",
    "(based on network.py by kvlinden)\n",
    "\n",
    "@author: ljh27\n",
    "\"\"\"\n",
    "\n",
    "from probability import BayesNet, enumeration_ask\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "# From AIMA code (probability.py)\n",
    "cloudyNet = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T: 0.10, F: 0.50}),\n",
    "    ('Rain', 'Cloudy', {T: 0.80, F: 0.20}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T): 0.99, (T, F): 0.90, (F, T): 0.90, (F, F): 0.00})\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part b:\n",
    "There are 4 variables, each with 2 possible values (True or False), so the number of values in the full joint probability distribution is 2^4 = 16\n",
    "\n",
    "Part c:\n",
    "The number of values in the Bayesian network is found by counting the number of values in the probability tables given in the diagram (Figure 14.12a), which in this case is 9\n",
    "\n",
    "Part d:\n",
    "_Hand calculations for these problems can be found in Homework2-HandCalculations.pdf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Cloudy):\n",
      "\tFalse: 0.5, True: 0.5\n",
      "\n",
      "P(Sprinkler | cloudy):\n",
      "\tFalse: 0.9, True: 0.1\n",
      "\n",
      "P(Cloudy | sprinkler ^ ¬rain):\n",
      "\tFalse: 0.952, True: 0.0476\n",
      "\n",
      "P(WetGrass | cloudy ∧ sprinkler ∧ rain):\n",
      "\tFalse: 0.01, True: 0.99\n",
      "\n",
      "P(Cloudy | ¬WetGrass):\n",
      "\tFalse: 0.639, True: 0.361\n"
     ]
    }
   ],
   "source": [
    "# i. P(Cloudy)\n",
    "print('P(Cloudy):')\n",
    "print('\\t' + enumeration_ask('Cloudy', dict(), cloudyNet).show_approx())\n",
    "\n",
    "# ii. P(Sprinkler | cloudy)\n",
    "print('\\nP(Sprinkler | cloudy):')\n",
    "print('\\t' + enumeration_ask('Sprinkler', dict(Cloudy=T), cloudyNet).show_approx())\n",
    "\n",
    "# iii. P(Cloudy | sprinkler ∧ ¬rain)\n",
    "print('\\nP(Cloudy | sprinkler ^ ¬rain):')\n",
    "print('\\t' + enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), cloudyNet).show_approx())\n",
    "\n",
    "# iv. P(WetGrass | cloudy ∧ sprinkler ∧ rain)\n",
    "print('\\nP(WetGrass | cloudy ∧ sprinkler ∧ rain):')\n",
    "print('\\t' + enumeration_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudyNet).show_approx())\n",
    "\n",
    "# v. P(Cloudy | ¬WetGrass)\n",
    "print('\\nP(Cloudy | ¬WetGrass):')\n",
    "print('\\t' + enumeration_ask('Cloudy', dict(WetGrass=F), cloudyNet).show_approx())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
